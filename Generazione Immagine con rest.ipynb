{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY4T6z6Mu30G"
      },
      "outputs": [],
      "source": [
        "import requests as rq\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image as mpimg\n",
        "\n",
        "BASE_URL = 'http://solaris.micc.unifi.it/andreaderosa'\n",
        "\n",
        "TARGET_TEXT=\"woman with long hair and crossed hand by leonardo da vinci\"\n",
        "USER_TEXT=\"with a mountain as background\"\n",
        "DIRECTORY=\"\"\n",
        "\n",
        "payload = {'prompt':USER_TEXT, 'directory':DIRECTORY, 'target_text':TARGET_TEXT}\n",
        "response = rq.get(BASE_URL, params=payload)\n",
        "\n",
        "json_values=response.json()\n",
        "\n",
        "rq_input=json_values['image']\n",
        "\n",
        "plt.imshow(rq_input)\n",
        "plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "url = \"http://solaris.micc.unifi.it/andreaderosa\"\n",
        "page = requests.get(url)\n",
        "print(page.status_code)\n",
        "\n",
        "#200 ok, 404 errore"
      ],
      "metadata": {
        "id": "IV3KXI0vVVUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from IPython.display import display\n",
        "\n",
        "from flask import Flask\n",
        "from flask import request\n",
        "import json\n",
        "\n",
        "app=Flask(__name__)\n",
        "\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def handle_request():\n",
        "\n",
        "  rq_prompt=str(request.args.get('prompt'))\n",
        "  rq_directory=str(request.args.get('directory'))\n",
        "  rq_target_text=str(request.args.get('target_text'))\n",
        "\n",
        "  TARGET_TEXT = rq_target_text\n",
        "  OUTPUT_DIR = rq_directory\n",
        "  prompt = rq_prompt\n",
        "\n",
        "  model_path = OUTPUT_DIR\n",
        "\n",
        "  scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, torch_dtype=torch.float16).to(\"cuda\")\n",
        "  target_embeddings = torch.load(os.path.join(model_path, \"target_embeddings.pt\")).to(\"cuda\")\n",
        "  optimized_embeddings = torch.load(os.path.join(model_path, \"optimized_embeddings.pt\")).to(\"cuda\")\n",
        "  g_cuda = None\n",
        "\n",
        "  g_cuda = torch.Generator(device='cuda')\n",
        "  seed = -1\n",
        "  g_cuda.manual_seed(seed)\n",
        "\n",
        "  num_samples = 1\n",
        "  guidance_scale = 5\n",
        "  num_inference_steps = 100\n",
        "  height = 512\n",
        "  width = 512\n",
        "\n",
        "  pipe.safety_checker = None\n",
        "  pipe.requires_safety_checker = False\n",
        "\n",
        "  with autocast(\"cuda\"), torch.inference_mode():\n",
        "      images = pipe(\n",
        "          TARGET_TEXT+\" \"+prompt,\n",
        "          height=height,\n",
        "          width=width,\n",
        "          num_images_per_prompt=num_samples,\n",
        "          num_inference_steps=num_inference_steps,\n",
        "          guidance_scale=guidance_scale,\n",
        "          generator=g_cuda\n",
        "      ).images\n",
        "\n",
        "  data_set={'image': images[0]}\n",
        "  json_dump=json.dumps(data_set)\n",
        "\n",
        "  return json_dump"
      ],
      "metadata": {
        "id": "FTYxFntnxicV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}